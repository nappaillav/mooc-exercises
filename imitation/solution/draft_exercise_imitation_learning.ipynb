{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "draft_exercise_imitation_learning.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rqz9sjGSawkn"
      },
      "outputs": [],
      "source": [
        "# requirement for running this assignment \n",
        "# gym-duckietown --> env and maps \n",
        "# Tqdm\n",
        "# Tensorboard\n",
        "\n",
        "################# TODO ######################\n",
        "# have to check the Tensorboard working with this implementation \n",
        "# Full running of the code from the notebook (locally)\n",
        "# On colab"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imitation Learning implementation"
      ],
      "metadata": {
        "id": "XZwJe7vnirMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import \n",
        "\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import os\n",
        "from typing import Tuple, List\n",
        "from gym_duckietown.simulator import AGENT_SAFETY_RAD\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision.models.resnet import conv3x3\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from torchvision.transforms import ToTensor, Normalize, Compose\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "from gym_duckietown.envs import DuckietownEnv\n",
        "import argparse\n"
      ],
      "metadata": {
        "id": "Qf9sjoGijV9V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Buffer"
      ],
      "metadata": {
        "id": "xL0dUVE2iziO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data util \n",
        "\n",
        "class MemoryMapDataset(Dataset):\n",
        "    \"\"\"Dataset to store multiple arrays on disk to avoid saturating the RAM\"\"\"\n",
        "\n",
        "    def __init__(self, size: int, data_size: tuple, target_size: tuple, path: str):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        size : int\n",
        "            Number of arrays to store, will be the first dimension of the resulting tensor dataset.\n",
        "        data_size : tuple\n",
        "            Size of data, may be 3D (CxHxW) for images or 2D/1D for features.\n",
        "        target_size : tuple\n",
        "            Size of the target, for our case it is a 1D array having, angular and linear speed.\n",
        "        path : str\n",
        "            Path where the file will be saved.\n",
        "        \"\"\"\n",
        "        self.size = size\n",
        "        self.data_size = data_size\n",
        "        self.target_size = target_size\n",
        "        self.path = path\n",
        "\n",
        "        # Path for each array\n",
        "        self.data_path = os.path.join(path, \"data.dat\")\n",
        "        self.target_path = os.path.join(path, \"target.dat\")\n",
        "\n",
        "        # Create arrays\n",
        "        self.data = np.memmap(self.data_path, dtype=\"float32\", mode=\"w+\", shape=(self.size, *self.data_size))\n",
        "        self.target = np.memmap(\n",
        "            self.target_path, dtype=\"float32\", mode=\"w+\", shape=(self.size, *self.target_size)\n",
        "        )\n",
        "\n",
        "        # Initialize number of saved records to zero\n",
        "        self.length = 0\n",
        "\n",
        "        # keep track of real length in case of bypassing size value\n",
        "        self.real_length = 0\n",
        "\n",
        "    def __getitem__(self, item) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "        \"\"\"Get one pair of training examples from the dataset.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        item : int\n",
        "            Index on the first dimension of the dataset.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        sample, target : tuple\n",
        "            Training sample consisting of data, label of data_size and target_size, respectively.\n",
        "        \"\"\"\n",
        "        sample = torch.tensor(self.data[item, ...])\n",
        "        target = torch.tensor(self.target[item, ...])\n",
        "\n",
        "        return sample, target\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        \"\"\"Get size (number of saved examples) of the dataset.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        length : int\n",
        "            Occupied length of the dataset. Note that it returns the number of saved examples rather than the maximum\n",
        "            size used in the initialization.\n",
        "        \"\"\"\n",
        "        return self.length\n",
        "\n",
        "    def extend(self, observations: List[np.ndarray], actions: List[np.ndarray]):\n",
        "        \"\"\"Saves observations to the dataset. Iterates through the lists containing matching pairs of observations and\n",
        "        actions. After saving each sample the dataset size is readjusted. If the dataset exceeds its maximum size\n",
        "        it will start overwriting the firs experiences.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        observations : List\n",
        "            List containing np.ndarray observations of size data_size.\n",
        "        actions\n",
        "            List containing np.ndarray actions of size target_size.\n",
        "        \"\"\"\n",
        "        for index, (observation, action) in enumerate(zip(observations, actions)):\n",
        "            current_data_indx = self.real_length + index\n",
        "            if self.real_length + index >= self.size:\n",
        "                # it will be a circular by getting rid of old experiments\n",
        "                current_data_indx %= self.size\n",
        "            self.data[current_data_indx, ...] = observation.astype(np.float32)\n",
        "            self.target[current_data_indx, ...] = action.astype(np.float32)\n",
        "        if self.real_length >= self.size:\n",
        "            self.length = self.size - 1\n",
        "        else:\n",
        "            self.length += len(observations)\n",
        "        self.real_length += len(observations)\n",
        "\n",
        "    def save(self):\n",
        "        \"\"\"In case of wanting to save the dataset this method should be implemented by flushing anc closing the memory\n",
        "        map. Note that the files (depending on the size parameter) may occupy considerable amount of memory.\n",
        "        \"\"\"\n",
        "        # TODO\n",
        "        pass\n"
      ],
      "metadata": {
        "id": "y-zCurHChNvr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Interative Imitation Learning Class "
      ],
      "metadata": {
        "id": "-lBAm5l5ipEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class InteractiveImitationLearning:\n",
        "    \"\"\"\n",
        "    A class used to contain main imitation learning algorithm\n",
        "    ...\n",
        "    Methods\n",
        "    -------\n",
        "    train(samples, debug)\n",
        "        start training imitation learning\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, teacher, learner, horizon, episodes, test=False):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        env :\n",
        "            duckietown environment\n",
        "        teacher :\n",
        "            expert used to train imitation learning\n",
        "        learner :\n",
        "            model used to learn\n",
        "        horizon : int\n",
        "            which is the number of observations to be collected during one episode\n",
        "        episode : int\n",
        "            number of episodes which is the number of collected trajectories\n",
        "        \"\"\"\n",
        "\n",
        "        self.environment = env\n",
        "        self.teacher = teacher\n",
        "        self.learner = learner\n",
        "        self.test = test\n",
        "\n",
        "        # from IIL\n",
        "        self._horizon = horizon\n",
        "        self._episodes = episodes\n",
        "\n",
        "        # data\n",
        "        self._observations = []\n",
        "        self._expert_actions = []\n",
        "\n",
        "        # statistics\n",
        "        self.learner_action = None\n",
        "        self.learner_uncertainty = None\n",
        "\n",
        "        self.teacher_action = None\n",
        "        self.active_policy = True  # if teacher is active\n",
        "\n",
        "        # internal count\n",
        "        self._current_horizon = 0\n",
        "        self._episode = 0\n",
        "\n",
        "        # event listeners\n",
        "        self._episode_done_listeners = []\n",
        "        self._found_obstacle = False\n",
        "        # steering angle gain\n",
        "        self.gain = 10\n",
        "\n",
        "    def train(self, debug=False):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        teacher :\n",
        "            expert used to train imitation learning\n",
        "        learner :\n",
        "            model used to learn\n",
        "        horizon : int\n",
        "            which is the number of observations to be collected during one episode\n",
        "        episode : int\n",
        "            number of episodes which is the number of collected trajectories\n",
        "        \"\"\"\n",
        "        self._debug = debug\n",
        "        for episode in range(self._episodes):\n",
        "            self._episode = episode\n",
        "            self._sampling()\n",
        "            self._optimize()  # episodic learning\n",
        "            self._on_episode_done()\n",
        "\n",
        "    def _sampling(self):\n",
        "        observation = self.environment.render_obs()\n",
        "        for horizon in range(self._horizon):\n",
        "            self._current_horizon = horizon\n",
        "            action = self._act(observation)\n",
        "            try:\n",
        "                next_observation, reward, done, info = self.environment.step(\n",
        "                    [action[0], action[1] * self.gain]\n",
        "                )\n",
        "            except Exception as e:\n",
        "                print(e)\n",
        "            if self._debug:\n",
        "                self.environment.render()\n",
        "            observation = next_observation\n",
        "\n",
        "    # execute current control policy\n",
        "    def _act(self, observation):\n",
        "        if self._episode <= 1:  # initial policy equals expert's\n",
        "            control_policy = self.teacher\n",
        "        else:\n",
        "            control_policy = self._mix()\n",
        "\n",
        "        control_action = control_policy.predict(observation)\n",
        "\n",
        "        self._query_expert(control_policy, control_action, observation)\n",
        "\n",
        "        self.active_policy = control_policy == self.teacher\n",
        "        if self.test:\n",
        "            return self.learner_action\n",
        "\n",
        "        return control_action\n",
        "\n",
        "    def _query_expert(self, control_policy, control_action, observation):\n",
        "        if control_policy == self.learner:\n",
        "            self.learner_action = control_action\n",
        "        else:\n",
        "            self.learner_action = self.learner.predict(observation)\n",
        "\n",
        "        if control_policy == self.teacher:\n",
        "            self.teacher_action = control_action\n",
        "        else:\n",
        "            self.teacher_action = self.teacher.predict(observation)\n",
        "\n",
        "        if self.teacher_action is not None:\n",
        "            self._aggregate(observation, self.teacher_action)\n",
        "\n",
        "        if self.teacher_action[0] < 0.1:\n",
        "            self._found_obstacle = True\n",
        "        else:\n",
        "            self._found_obstacle = False\n",
        "\n",
        "    def _mix(self):\n",
        "        raise NotImplementedError()\n",
        "\n",
        "    def _aggregate(self, observation, action):\n",
        "        if not (self.test):\n",
        "            self._observations.append(observation)\n",
        "            self._expert_actions.append(action)\n",
        "\n",
        "    def _optimize(self):\n",
        "        if not (self.test):\n",
        "            self.learner.optimize(self._observations, self._expert_actions, self._episode)\n",
        "            print(\"saving model\")\n",
        "            self.learner.save()\n",
        "\n",
        "    # TRAINING EVENTS\n",
        "\n",
        "    # triggered after an episode of learning is done\n",
        "    def on_episode_done(self, listener):\n",
        "        self._episode_done_listeners.append(listener)\n",
        "\n",
        "    def _on_episode_done(self):\n",
        "        for listener in self._episode_done_listeners:\n",
        "            listener.episode_done(self._episode)\n",
        "        self.environment.reset()"
      ],
      "metadata": {
        "id": "u3wlPsijiSoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Dagger Class"
      ],
      "metadata": {
        "id": "e3NLE3MRjNvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The Dagger Class is the important, which uses Interative Inmitation learnign \n",
        "* The interative imitation learning has most of the curcial functions implemented\n",
        "* In my opinion we can move those functions to Dagger class.\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "class DAgger(InteractiveImitationLearning):\n",
        "    \"\"\"\n",
        "    DAgger algorithm to mix policies between learner and expert\n",
        "    Ross, StÃ©phane, Geoffrey Gordon, and Drew Bagnell. \"A reduction of imitation learning and structured prediction to no-regret online learning.\" Proceedings of the fourteenth international conference on artificial intelligence and statistics. 2011.\n",
        "    ...\n",
        "    Methods\n",
        "    -------\n",
        "    _mix\n",
        "        used to return a policy teacher / expert based on random choice and safety checks\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, env, teacher, learner, horizon, episodes, alpha=0.5, test=False):\n",
        "        InteractiveImitationLearning.__init__(self, env, teacher, learner, horizon, episodes, test)\n",
        "        # expert decay\n",
        "        self.p = alpha\n",
        "        self.alpha = self.p\n",
        "\n",
        "        # thresholds used to give control back to learner once the teacher converges\n",
        "        self.convergence_distance = 0.05\n",
        "        self.convergence_angle = np.pi / 18\n",
        "\n",
        "        # threshold on angle and distance from the lane when using the model to avoid going off track and env reset within an episode\n",
        "        self.angle_limit = np.pi / 8\n",
        "        self.distance_limit = 0.12\n",
        "\n",
        "    def _mix(self):\n",
        "        control_policy = np.random.choice(a=[self.teacher, self.learner], p=[self.alpha, 1.0 - self.alpha])\n",
        "        if self._found_obstacle:\n",
        "            return self.teacher\n",
        "        try:\n",
        "            lp = self.environment.get_lane_pos2(self.environment.cur_pos, self.environment.cur_angle)\n",
        "        except:\n",
        "            return control_policy\n",
        "        if self.active_policy:\n",
        "            # keep using tecaher untill duckiebot converges back on track\n",
        "            if not (abs(lp.dist) < self.convergence_distance and abs(lp.angle_rad) < self.convergence_angle):\n",
        "                return self.teacher\n",
        "        else:\n",
        "            # in case we are using our learner and it started to diverge a lot we need to give\n",
        "            # control back to the expert\n",
        "            if abs(lp.dist) > self.distance_limit or abs(lp.angle_rad) > self.angle_limit:\n",
        "                return self.teacher\n",
        "        return control_policy\n",
        "\n",
        "    def _on_episode_done(self):\n",
        "        self.alpha = self.p ** self._episode\n",
        "        # Clear experience\n",
        "        self._observations = []\n",
        "        self._expert_actions = []\n",
        "\n",
        "        InteractiveImitationLearning._on_episode_done(self)"
      ],
      "metadata": {
        "id": "7_WRtEWqjRfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Teacher (Pure Pursuit policy)\n"
      ],
      "metadata": {
        "id": "ZK8vJcaMn8R0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "POSITION_THRESHOLD = 0.04\n",
        "REF_VELOCITY = 0.7\n",
        "FOLLOWING_DISTANCE = 0.24\n",
        "AGENT_SAFETY_GAIN = 1.15\n",
        "\n",
        "\n",
        "class PurePursuitPolicy:\n",
        "    \"\"\"\n",
        "    A Pure Pusuit controller class to act as an expert to the model\n",
        "    ...\n",
        "    Methods\n",
        "    -------\n",
        "    forward(images)\n",
        "        makes a model forward pass on input images\n",
        "\n",
        "    loss(*args)\n",
        "        takes images and target action to compute the loss function used in optimization\n",
        "\n",
        "    predict(observation)\n",
        "        takes an observation image and predicts using env information the action\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self, env, ref_velocity=REF_VELOCITY, following_distance=FOLLOWING_DISTANCE, max_iterations=1000\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        ref_velocity : float\n",
        "            duckiebot maximum velocity (default 0.7)\n",
        "        following_distance : float\n",
        "            distance used to follow the trajectory in pure pursuit (default 0.24)\n",
        "        \"\"\"\n",
        "        self.env = env\n",
        "        self.following_distance = following_distance\n",
        "        self.max_iterations = max_iterations\n",
        "        self.ref_velocity = ref_velocity\n",
        "\n",
        "    def predict(self, observation):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        observation : image\n",
        "            image of current observation from simulator\n",
        "        Returns\n",
        "        -------\n",
        "        action: list\n",
        "            action having velocity and omega of current observation\n",
        "        \"\"\"\n",
        "        closest_point, closest_tangent = self.env.unwrapped.closest_curve_point(\n",
        "            self.env.cur_pos, self.env.cur_angle\n",
        "        )\n",
        "        if closest_point is None or closest_tangent is None:\n",
        "            self.env.reset()\n",
        "            closest_point, closest_tangent = self.env.unwrapped.closest_curve_point(\n",
        "                self.env.cur_pos, self.env.cur_angle\n",
        "            )\n",
        "\n",
        "        current_world_objects = self.env.objects\n",
        "        # to slow down if there's a duckiebot in front of you\n",
        "        # this is used to avoid hitting another moving duckiebot in the map\n",
        "        # in case of training LFV baseline\n",
        "        velocity_slow_down = 1\n",
        "        for obj in current_world_objects:\n",
        "            if not obj.static and obj.kind == \"duckiebot\":\n",
        "                if True:\n",
        "                    collision_penalty = abs(\n",
        "                        obj.proximity(self.env.cur_pos, AGENT_SAFETY_RAD * AGENT_SAFETY_GAIN)\n",
        "                    )\n",
        "                    if collision_penalty > 0:\n",
        "                        # this means we are approaching and we need to slow down\n",
        "                        velocity_slow_down = collision_penalty\n",
        "                        break\n",
        "\n",
        "        lookup_distance = self.following_distance\n",
        "        # projected_angle used to detect corners and to reduce the velocity accordingly\n",
        "        projected_angle, _, _ = self._get_projected_angle_difference(0.3)\n",
        "        velocity_scale = 1\n",
        "\n",
        "        current_tile_pos = self.env.get_grid_coords(self.env.cur_pos)\n",
        "        current_tile = self.env._get_tile(*current_tile_pos)\n",
        "        if \"curve\" in current_tile[\"kind\"] or abs(projected_angle) < 0.92:\n",
        "            # slowing down by a scale of 0.5\n",
        "            velocity_scale = 0.5\n",
        "        _, closest_point, curve_point = self._get_projected_angle_difference(lookup_distance)\n",
        "\n",
        "        if closest_point is None:  # if cannot find a curve point in max iterations\n",
        "            return [0, 0]\n",
        "\n",
        "        # Compute a normalized vector to the curve point\n",
        "        point_vec = curve_point - self.env.cur_pos\n",
        "        point_vec /= np.linalg.norm(point_vec)\n",
        "        right_vec = np.array([math.sin(self.env.cur_angle), 0, math.cos(self.env.cur_angle)])\n",
        "        dot = np.dot(right_vec, point_vec)\n",
        "        omega = -1 * dot\n",
        "        # range of dot is just -pi/2 and pi/2 and will be multiplied later by a gain adjustable if we are testing on a hardware or not\n",
        "        velocity = self.ref_velocity * velocity_scale\n",
        "        if velocity_slow_down < 0.2:\n",
        "            velocity = 0\n",
        "            omega = 0\n",
        "\n",
        "        action = [velocity, omega]\n",
        "\n",
        "        return action\n",
        "\n",
        "    def _get_projected_angle_difference(self, lookup_distance):\n",
        "        # Find the projection along the path\n",
        "        closest_point, closest_tangent = self.env.closest_curve_point(self.env.cur_pos, self.env.cur_angle)\n",
        "\n",
        "        iterations = 0\n",
        "        curve_angle = None\n",
        "\n",
        "        while iterations < 10:\n",
        "            # Project a point ahead along the curve tangent,\n",
        "            # then find the closest point to to that\n",
        "            follow_point = closest_point + closest_tangent * lookup_distance\n",
        "            curve_point, curve_angle = self.env.closest_curve_point(follow_point, self.env.cur_angle)\n",
        "\n",
        "            # If we have a valid point on the curve, stop\n",
        "            if curve_angle is not None and curve_point is not None:\n",
        "                break\n",
        "\n",
        "            iterations += 1\n",
        "            lookup_distance *= 0.5\n",
        "\n",
        "        if curve_angle is None:  # if cannot find a curve point in max iterations\n",
        "            return None, None, None\n",
        "\n",
        "        else:\n",
        "            return np.dot(curve_angle, closest_tangent), closest_point, curve_point"
      ],
      "metadata": {
        "id": "2HDWjWt3n7we"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "RO637biloKhF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "The model learns from color image, but here we can try more variety\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "class Flatten(nn.Module):\n",
        "    def forward(self, x):\n",
        "        return x.view(x.shape[0], -1)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, inplanes, planes, stride=1):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
        "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn1 = nn.BatchNorm2d(planes)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        self.conv2 = conv3x3(inplanes, planes, stride)\n",
        "        self.bn2 = nn.BatchNorm2d(planes)\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv1 = self.conv1(x)\n",
        "        conv1 = self.bn1(conv1)\n",
        "        conv1 = self.relu(conv1)\n",
        "\n",
        "        conv2 = self.conv2(x)\n",
        "        conv2 = self.bn2(conv2)\n",
        "\n",
        "        return conv1 + conv2\n",
        "\n",
        "\n",
        "class Dronet(nn.Module):\n",
        "    \"\"\"\n",
        "    A class used to define action regressor model based on Dronet arch.\n",
        "    Loquercio, Antonio, et al. \"Dronet: Learning to fly by driving.\" IEEE Robotics and Automation Letters 3.2 (2018): 1088-1095.\n",
        "    ...\n",
        "    Methods\n",
        "    -------\n",
        "    forward(images)\n",
        "        makes a model forward pass on input images\n",
        "    loss(*args)\n",
        "        takes images and target action to compute the loss function used in optimization\n",
        "    predict(*args)\n",
        "        takes images as input and predict the action space unnormalized\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_outputs=2, max_velocity=0.7, max_steering=np.pi / 2):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        num_outputs : int\n",
        "            number of outputs of the action space (default 2)\n",
        "        max_velocity : float\n",
        "            the maximum velocity used by the teacher (default 0.7)\n",
        "        max_steering : float\n",
        "            maximum steering angle as we are predicting normalized [0-1] (default pi/2)\n",
        "        \"\"\"\n",
        "        super(Dronet, self).__init__()\n",
        "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.num_outputs = num_outputs\n",
        "        self.feature_extractor = nn.Sequential(\n",
        "            nn.Conv2d(3, 32, kernel_size=5, stride=2),\n",
        "            nn.MaxPool2d(kernel_size=(3, 3), stride=[2, 2]),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            BasicBlock(32, 32, stride=2),\n",
        "            nn.BatchNorm2d(32),\n",
        "            nn.ReLU(inplace=True),\n",
        "            BasicBlock(32, 64, stride=2),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(inplace=True),\n",
        "            BasicBlock(64, 128, stride=2),\n",
        "            Flatten(),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(0.5),\n",
        "        )\n",
        "\n",
        "        self.num_feats_extracted = 2560\n",
        "        # predicting steering angle\n",
        "        self.steering_angle_channel = nn.Sequential(nn.Linear(self.num_feats_extracted, 1))\n",
        "\n",
        "        # predicting if the bot should speed up or slow down\n",
        "        self.speed_up_channel = nn.Sequential(nn.Linear(self.num_feats_extracted, 1))\n",
        "\n",
        "        # Decaying speed up loss parameters\n",
        "        self.decay = 1 / 10\n",
        "        self.epoch_0 = 10\n",
        "        self.epoch = 0\n",
        "\n",
        "        # Max steering angle, minimum velocity and maximum velocity parameters\n",
        "        self.max_steering = max_steering\n",
        "        self.max_velocity = max_velocity\n",
        "        self.max_velocity_tensor = torch.tensor(self.max_velocity).to(self._device)\n",
        "        self.min_velocity = self.max_velocity * 0.5\n",
        "        self.min_velocity_tensor = torch.tensor(self.min_velocity).to(self._device)\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        images : tensor\n",
        "            batch of input images to get normalized predicted action\n",
        "        Returns\n",
        "        -------\n",
        "        action: tensor\n",
        "            normalized predicted action from the model\n",
        "        \"\"\"\n",
        "        features = self.feature_extractor(images)\n",
        "        steering_angle = self.steering_angle_channel(features)\n",
        "        is_speed_up = self.speed_up_channel(features)\n",
        "        return is_speed_up, steering_angle\n",
        "\n",
        "    def loss(self, *args):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        *args :\n",
        "            takes batch of images and target action space to get the loss function.\n",
        "        Returns\n",
        "        -------\n",
        "        loss: tensor\n",
        "            loss function used by the optimizer to update the model\n",
        "        \"\"\"\n",
        "        self.train()\n",
        "        images, target = args\n",
        "        is_speed_up, steering_angle = self.forward(images)\n",
        "        criterion_v = nn.BCEWithLogitsLoss()\n",
        "        speed_up = (\n",
        "            (target[:, 0] > self.min_velocity).float().unsqueeze(1)\n",
        "        )  # 0 for expert speeding up and 1 for slowing down for a corner or an incoming duckbot\n",
        "        loss_steering_angle = F.mse_loss(steering_angle, target[:, 1].unsqueeze(1), reduction=\"mean\")\n",
        "        loss_v = criterion_v(is_speed_up, speed_up)\n",
        "        loss = loss_steering_angle + loss_v * max(0, 1 - np.exp(self.decay * (self.epoch - self.epoch_0)))\n",
        "        return loss\n",
        "\n",
        "    def predict(self, *args):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        *args : tensor\n",
        "            batch of input images to get unnormalized predicted action\n",
        "        Returns\n",
        "        -------\n",
        "        action: tensor\n",
        "            action having velocity and omega of shape (batch_size, 2)\n",
        "        \"\"\"\n",
        "        images = args[0]\n",
        "        is_speed_up, steering_angle = self.forward(images)\n",
        "        is_speed_up = torch.sigmoid(is_speed_up)\n",
        "        v_tensor = (is_speed_up) * self.max_velocity_tensor + (1 - is_speed_up) * self.min_velocity_tensor\n",
        "        steering_angle = steering_angle * self.max_steering\n",
        "        output = torch.cat((v_tensor, steering_angle), 1).squeeze().detach()\n",
        "        return output.cpu().numpy()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    batch_size = 2\n",
        "    img_size = (120, 160)\n",
        "    model = Dronet()\n",
        "    input_image = torch.rand((batch_size, 3, img_size[0], img_size[1])).to(model._device)\n",
        "    prediction = model.predict(input_image)\n",
        "    assert list(prediction.shape) == [batch_size, model.num_outputs]"
      ],
      "metadata": {
        "id": "8CJmgTxwoKRK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neural Network Policy"
      ],
      "metadata": {
        "id": "cb6-GuOKoxlv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "class NeuralNetworkPolicy:\n",
        "    \"\"\"\n",
        "    A wrapper to train neural network model\n",
        "    ...\n",
        "    Methods\n",
        "    -------\n",
        "    optimize(observations, expert_actions, episode)\n",
        "        train the model on the newly collected data from the simulator\n",
        "\n",
        "    predict(observation)\n",
        "        takes images and predicts the action space using the model\n",
        "\n",
        "    save\n",
        "        save a model checkpoint to storage location\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, optimizer, storage_location, dataset, **kwargs):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        model :\n",
        "            input pytorch model that will be trained\n",
        "        optimizer :\n",
        "            torch optimizer\n",
        "        storage_location : string\n",
        "            path of the model to be saved , the dataset and tensorboard logs\n",
        "        dataset :\n",
        "            object storing observation and labels from expert\n",
        "        \"\"\"\n",
        "        self._train_iteration = 0\n",
        "        self._device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        # Base parameters\n",
        "        self.model = model.to(self._device)\n",
        "        self.optimizer = optimizer\n",
        "        self.storage_location = storage_location\n",
        "        self.writer = SummaryWriter(self.storage_location)\n",
        "\n",
        "        # Optional parameters\n",
        "        self.epochs = kwargs.get(\"epochs\", 10)\n",
        "        self.batch_size = kwargs.get(\"batch_size\", 32)\n",
        "        self.input_shape = kwargs.get(\"input_shape\", (60, 80))\n",
        "        self.max_velocity = kwargs.get(\"max_velocity\", 0.7)\n",
        "\n",
        "        self.episode = 0\n",
        "\n",
        "        self.dataset = dataset\n",
        "        # Load previous weights\n",
        "        if \"model_path\" in kwargs:\n",
        "            self.model.load_state_dict(torch.load(kwargs.get(\"model_path\"), map_location=self._device))\n",
        "            print(\"Loaded \")\n",
        "\n",
        "    def __del__(self):\n",
        "        self.writer.close()\n",
        "\n",
        "    def optimize(self, observations, expert_actions, episode):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        observations :\n",
        "            input images collected from the simulator\n",
        "        expert_actions :\n",
        "            list of actions each action [velocity, steering_angle] from expert\n",
        "        episode : int\n",
        "            current episode number\n",
        "        \"\"\"\n",
        "        print(\"Starting episode #\", str(episode))\n",
        "        self.episode = episode\n",
        "        self.model.episode = episode\n",
        "        # Transform newly received data\n",
        "        observations, expert_actions = self._transform(observations, expert_actions)\n",
        "\n",
        "        # Retrieve data loader\n",
        "        dataloader = self._get_dataloader(observations, expert_actions)\n",
        "        # Train model\n",
        "        for epoch in tqdm(range(1, self.epochs + 1)):\n",
        "            running_loss = 0.0\n",
        "            self.model.epoch = epoch\n",
        "            for i, data in enumerate(dataloader, 0):\n",
        "                # Send data to device\n",
        "                data = [variable.to(self._device) for variable in data]\n",
        "\n",
        "                # zero the parameter gradients\n",
        "                self.optimizer.zero_grad()\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                loss = self.model.loss(*data)\n",
        "                loss.backward()\n",
        "                self.optimizer.step()\n",
        "\n",
        "                # Statistics\n",
        "                running_loss += loss.item()\n",
        "\n",
        "            # Logging\n",
        "            self._logging(loss=running_loss, epoch=epoch)\n",
        "\n",
        "        # Post training routine\n",
        "        self._on_optimization_end()\n",
        "\n",
        "    def predict(self, observation):\n",
        "        \"\"\"\n",
        "        Parameters\n",
        "        ----------\n",
        "        observations :\n",
        "            input image from the simulator\n",
        "        Returns\n",
        "        ----------\n",
        "        prediction:\n",
        "            action space of input observation\n",
        "        \"\"\"\n",
        "        # Apply transformations to data\n",
        "        observation, _ = self._transform([observation], [0])\n",
        "        observation = torch.tensor(observation)\n",
        "        # Predict with model\n",
        "        prediction = self.model.predict(observation.to(self._device))\n",
        "\n",
        "        return prediction\n",
        "\n",
        "    def save(self):\n",
        "        torch.save(self.model.state_dict(), os.path.join(self.storage_location, \"model.pt\"))\n",
        "\n",
        "    def _transform(self, observations, expert_actions):\n",
        "        # Resize images\n",
        "        observations = [\n",
        "            Image.fromarray(cv2.resize(observation, dsize=self.input_shape[::-1]))\n",
        "            for observation in observations\n",
        "        ]\n",
        "        # Transform to tensors\n",
        "        compose_obs = Compose(\n",
        "            [\n",
        "                ToTensor(),\n",
        "                Normalize(\n",
        "                    (0.485, 0.456, 0.406), (0.229, 0.224, 0.225)\n",
        "                ),  # using imagenet normalization values\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        observations = [compose_obs(observation).cpu().numpy() for observation in observations]\n",
        "        try:\n",
        "            # Scaling steering angle to become in range -1 to 1 to make it easier to regress\n",
        "            # Scaling velocity to range 0-1 based on max velocity\n",
        "            expert_actions = [\n",
        "                np.array([expert_action[0] / self.max_velocity, expert_action[1] / (np.pi / 2)])\n",
        "                for expert_action in expert_actions\n",
        "            ]\n",
        "        except:\n",
        "            pass\n",
        "        expert_actions = [torch.tensor(expert_action).cpu().numpy() for expert_action in expert_actions]\n",
        "\n",
        "        return observations, expert_actions\n",
        "\n",
        "    def _get_dataloader(self, observations, expert_actions):\n",
        "        # Include new experiences\n",
        "        self.dataset.extend(observations, expert_actions)\n",
        "        dataloader = DataLoader(self.dataset, batch_size=self.batch_size, shuffle=True)\n",
        "        return dataloader\n",
        "\n",
        "    def _logging(self, **kwargs):\n",
        "        epoch = kwargs.get(\"epoch\")\n",
        "        loss = kwargs.get(\"loss\")\n",
        "        self.writer.add_scalar(\"Loss/train/{}\".format(self._train_iteration), loss, epoch)\n",
        "\n",
        "    def _on_optimization_end(self):\n",
        "        self._train_iteration += 1"
      ],
      "metadata": {
        "id": "rUsNy-DOo8az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Main function"
      ],
      "metadata": {
        "id": "T1Z9cPF9pg9Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "def launch_env(map_name, randomize_maps_on_reset=False, domain_rand=False):\n",
        "    environment = DuckietownEnv(\n",
        "        domain_rand=domain_rand,\n",
        "        max_steps=math.inf,\n",
        "        map_name=map_name,\n",
        "        randomize_maps_on_reset=False,\n",
        "    )\n",
        "    return environment\n",
        "\n",
        "\n",
        "def teacher(env, max_velocity):\n",
        "    return PurePursuitPolicy(env=env, ref_velocity=max_velocity)\n",
        "\n",
        "\n",
        "def process_args():\n",
        "    parser = argparse.ArgumentParser()\n",
        "    parser.add_argument(\"--episode\", \"-i\", default=10, type=int)\n",
        "    parser.add_argument(\"--horizon\", \"-r\", default=128, type=int)\n",
        "    parser.add_argument(\"--learning-rate\", \"-l\", default=2, type=int)\n",
        "    parser.add_argument(\"--decay\", \"-d\", default=2, type=int)\n",
        "    parser.add_argument(\"--save-path\", \"-s\", default=\"iil_baseline\", type=str)\n",
        "    parser.add_argument(\"--map-name\", \"-m\", default=\"loop_empty\", type=str)\n",
        "    parser.add_argument(\"--num-outputs\", \"-n\", default=2, type=int)\n",
        "    parser.add_argument(\"--domain-rand\", \"-dr\", action=\"store_true\")\n",
        "    parser.add_argument(\"--randomize-map\", \"-rm\", action=\"store_true\")\n",
        "    return parser\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    parser = process_args()\n",
        "    input_shape = (120, 160)\n",
        "    batch_size = 16\n",
        "    epochs = 10\n",
        "    learning_rates = [1e-1, 1e-2, 1e-3, 1e-4, 1e-5]\n",
        "    # decays\n",
        "    mixing_decays = [0.5, 0.6, 0.7, 0.8, 0.85, 0.9, 0.95]\n",
        "    # Max velocity\n",
        "    max_velocity = 0.5\n",
        "\n",
        "    config = parser.parse_args()\n",
        "    # check for  storage path\n",
        "    if not (os.path.isdir(config.save_path)):\n",
        "        os.makedirs(config.save_path)\n",
        "    # launching environment\n",
        "    environment = launch_env(\n",
        "        config.map_name,\n",
        "        domain_rand=config.domain_rand,\n",
        "        randomize_maps_on_reset=config.randomize_map,\n",
        "    )\n",
        "\n",
        "    task_horizon = config.horizon\n",
        "    task_episode = config.episode\n",
        "\n",
        "    model = Dronet(num_outputs=config.num_outputs, max_velocity=max_velocity)\n",
        "    policy_optimizer = torch.optim.Adam(model.parameters(), lr=learning_rates[config.learning_rate])\n",
        "\n",
        "    dataset = MemoryMapDataset(25000, (3, *input_shape), (2,), config.save_path)\n",
        "    learner = NeuralNetworkPolicy(\n",
        "        model=model,\n",
        "        optimizer=policy_optimizer,\n",
        "        storage_location=config.save_path,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        input_shape=input_shape,\n",
        "        max_velocity=max_velocity,\n",
        "        dataset=dataset,\n",
        "    )\n",
        "\n",
        "    algorithm = DAgger(\n",
        "        env=environment,\n",
        "        teacher=teacher(environment, max_velocity),\n",
        "        learner=learner,\n",
        "        horizon=task_horizon,\n",
        "        episodes=task_episode,\n",
        "        alpha=mixing_decays[config.decay],\n",
        "    )\n",
        "\n",
        "    algorithm.train(debug=True)  # DEBUG to show simulation\n",
        "\n",
        "    environment.close()"
      ],
      "metadata": {
        "id": "PUQO38vipggD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}